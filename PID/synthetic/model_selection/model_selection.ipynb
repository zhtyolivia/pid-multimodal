{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "sys.path.append(os.getcwd())\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "from rus import *\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### process results csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dataset', ' model', ' R', ' U1', ' U2', ' S', ' acc'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('synthetic/model_selection/results.csv')\n",
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS = np.unique(df['dataset'])\n",
    "METHODS = ['agree', 'align', 'early_fusion', 'elem', 'mfm', 'mi', 'mult', 'outer', 'lower']\n",
    "MEASURES = ['redundancy', 'unique1', 'unique2', 'synergy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dict()\n",
    "results = dict()\n",
    "for setting in SETTINGS:\n",
    "    results[setting] = dict()\n",
    "for i in range(len(df)):\n",
    "    entry = df.loc[i]\n",
    "    if entry[' model'] == ' dataset':\n",
    "        datasets[entry['dataset'].strip()] = {'redundancy':entry[' R'], 'unique1':entry[' U1'], 'unique2':entry[' U2'], 'synergy':entry[' S']}\n",
    "    else:\n",
    "        results[entry['dataset'].strip()][entry[' model'].strip().lower()] = {'redundancy':entry[' R'], 'unique1':entry[' U1'], 'unique2':entry[' U2'], 'synergy':entry[' S'], 'acc':entry[' acc']}\n",
    "    \n",
    "with open('synthetic/model_selection/results_affect.pickle', 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "with open('synthetic/model_selection/datasets_affect.pickle', 'wb') as f:\n",
    "    pickle.dump(datasets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['enrico', 'humor01', 'humor02', 'humor12', 'mimic', 'mosei01', 'mosei02', 'mosei12', 'sarcasm01', 'sarcasm02', 'sarcasm12'])\n",
      "dict_keys(['mimic', 'enrico', 'humor12', 'humor01', 'humor02', 'mosei12', 'mosei01', 'mosei02', 'sarcasm01', 'sarcasm02', 'sarcasm12'])\n"
     ]
    }
   ],
   "source": [
    "with open('synthetic/model_selection/results_affect.pickle', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "print(results.keys())\n",
    "with open('synthetic/model_selection/datasets_affect.pickle', 'rb') as f:\n",
    "    datasets = pickle.load(f)\n",
    "print(datasets.keys())\n",
    "normalized_results = dict()\n",
    "normalized_datasets = dict()\n",
    "accs = []\n",
    "for setting in SETTINGS:\n",
    "    normalized_results[setting] = dict()\n",
    "    for method in results[setting]:\n",
    "        normalized_results[setting][method] = dict()\n",
    "        normalized_datasets[setting] = dict()\n",
    "        results_total = sum([results[setting][method][measure] for measure in results[setting][method]])\n",
    "        datasets_total = sum([datasets[setting][measure] for measure in datasets[setting]])\n",
    "        for measure in datasets[setting]:\n",
    "            normalized_results[setting][method][measure] = results[setting][method][measure] / results_total\n",
    "            normalized_datasets[setting][measure] = datasets[setting][measure] / datasets_total\n",
    "        accs.append(results[setting][method]['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mimic {'redundancy': 0.0513069475394659, 'unique1': 0.1708080626662141, 'unique2': 2.719328957684913e-08, 'synergy': 0.0056557956533846}\n",
      "enrico {'redundancy': 0.7305898871257488, 'unique1': 0.3756550453895989, 'unique2': 0.5295818376620958, 'synergy': 0.3351161376596552}\n",
      "humor12 {'redundancy': 0.0074837124438409, 'unique1': 0.0021428311352223, 'unique2': 0.0018722723944138, 'synergy': 0.0380261035251354}\n",
      "humor01 {'redundancy': 0.0096185016837852, 'unique1': 0.0215670453302197, 'unique2': 8.041879712466572e-06, 'synergy': 0.1739730317062034}\n",
      "humor02 {'redundancy': 0.0093378549487458, 'unique1': 0.0218476920659914, 'unique2': 1.81299020742313e-05, 'synergy': 0.1684954904734209}\n",
      "mosei12 {'redundancy': 0.0122012771897019, 'unique1': 5.1115889436551705e-15, 'unique2': 0.0079621148751152, 'synergy': 0.0237073327557628}\n",
      "mosei01 {'redundancy': 0.0119234634791755, 'unique1': 0.0047424614051067, 'unique2': 0.0002778137128733, 'synergy': 0.0227675033158421}\n",
      "mosei02 {'redundancy': 0.0148715538433674, 'unique1': 0.0017943710425773, 'unique2': 0.0052918382216908, 'synergy': 0.0300468882558571}\n",
      "sarcasm01 {'redundancy': 0.13897798270621, 'unique1': 0.0118777628230425, 'unique2': 0.0148744113905467, 'synergy': 0.2034366562372524}\n",
      "sarcasm02 {'redundancy': 0.138674655450335, 'unique1': 0.0107346276133233, 'unique2': 0.0151802035795652, 'synergy': 0.3358682702120688}\n",
      "sarcasm12 {'redundancy': 0.1430384703699769, 'unique1': 0.0063667685202978, 'unique2': 0.0078140594910095, 'synergy': 0.3671213343146194}\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(dataset, datasets[dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mimic [(0.9144696505211528, 'outer'), (0.91477621091355, 'align'), (0.9163090128755365, 'additive'), (0.9163090128755365, 'mi'), (0.9172286940527284, 'agree')]\n",
      "enrico [(0.496575342465753, 'outer'), (0.5, 'early_fusion'), (0.5068493150684932, 'agree'), (0.517123287671232, 'align'), (0.5205479452054794, 'lower')]\n",
      "humor12 [(0.6030245746691871, 'lower'), (0.615311909262759, 'mfm'), (0.619092627599243, 'outer'), (0.620982986767485, 'mi'), (0.636105860113421, 'elem')]\n",
      "humor01 [(0.592627599243856, 'early_fusion'), (0.594517958412098, 'elem'), (0.595463137996219, 'mfm'), (0.598298676748582, 'outer'), (0.609640831758034, 'mi')]\n",
      "humor02 [(0.622873345935727, 'outer'), (0.623818525519848, 'early_fusion'), (0.624763705103969, 'lower'), (0.64461247637051, 'mi'), (0.648393194706994, 'mfm')]\n",
      "mosei12 [(0.7984602694528458, 'outer'), (0.799285125103107, 'additive'), (0.800659884520209, 'elem'), (0.8012097882870498, 'agree'), (0.8100082485565027, 'mi')]\n",
      "mosei01 [(0.6464118779213638, 'additive'), (0.6469617816882046, 'mi'), (0.6491613967555677, 'outer'), (0.6505361561726698, 'elem'), (0.6521858674731922, 'lower')]\n",
      "mosei02 [(0.7962606543854825, 'lower'), (0.7979103656860049, 'additive'), (0.8014847401704702, 'agree'), (0.8064338740720374, 'elem'), (0.810283200439923, 'outer')]\n",
      "sarcasm01 [(0.579710144927536, 'agree'), (0.58695652173913, 'lower'), (0.601449275362318, 'outer'), (0.608695652173913, 'additive'), (0.615942028985507, 'align')]\n",
      "sarcasm02 [(0.630434782608695, 'mi'), (0.637681159420289, 'elem'), (0.666666666666666, 'agree'), (0.688405797101449, 'additive'), (0.695652173913043, 'align')]\n",
      "sarcasm12 [(0.601449275362318, 'elem'), (0.630434782608695, 'mi'), (0.681159420289855, 'agree'), (0.702898550724637, 'additive'), (0.702898550724637, 'align')]\n"
     ]
    }
   ],
   "source": [
    "model_selection_best = dict()\n",
    "for setting in datasets:\n",
    "    measures = []\n",
    "    for method in results[setting]:\n",
    "        measures.append((results[setting][method]['acc'], method))\n",
    "    measures = sorted(measures)\n",
    "    model_selection_best[setting] = results[setting][measures[-1][1]]['acc']\n",
    "    print(setting, measures[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('synthetic/experiments2/datasets.pickle', 'rb') as f:\n",
    "    old_datasets = pickle.load(f)\n",
    "with open('synthetic/experiments2/results.pickle', 'rb') as f:\n",
    "    old_results = pickle.load(f)\n",
    "normalized_old_datasets = dict()\n",
    "for setting in old_datasets:\n",
    "    normalized_old_datasets[setting] = dict()\n",
    "    datasets_total = sum([old_datasets[setting][measure] for measure in MEASURES])\n",
    "    for measure in MEASURES:\n",
    "        normalized_old_datasets[setting][measure] = old_datasets[setting][measure] / datasets_total\n",
    "methods = ['additive', 'agree', 'align', 'early_fusion', 'elem', 'recon', 'mi', 'mult', 'outer', 'lower']\n",
    "model_selection = dict()\n",
    "for setting in old_datasets:\n",
    "    measures = sorted([(old_results[method][setting]['acc'], method) for method in methods])\n",
    "    model_selection[setting] = measures[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mimic selected models [(0.7316666666666667, 'outer'), (0.7338888888888889, 'early_fusion'), (0.7361111111111112, 'recon')]\n",
      "99.69919786096257\n",
      "enrico selected models [(0.7138888888888889, 'additive'), (0.7183333333333334, 'align'), (0.72, 'agree')]\n",
      "99.34210526315772\n",
      "humor12 selected models [(0.5461111111111111, 'recon'), (0.5538888888888889, 'align'), (0.555, 'mult')]\n",
      "96.73105497771168\n",
      "humor01 selected models [(0.5461111111111111, 'recon'), (0.5538888888888889, 'align'), (0.555, 'mult')]\n",
      "97.67441860465112\n",
      "humor02 selected models [(0.5461111111111111, 'recon'), (0.5538888888888889, 'align'), (0.555, 'mult')]\n",
      "100.0\n",
      "mosei12 selected models [(0.6477777777777778, 'agree'), (0.6488888888888888, 'align'), (0.6488888888888888, 'recon')]\n",
      "98.91378139850644\n",
      "mosei01 selected models [(0.6844444444444444, 'early_fusion'), (0.685, 'recon'), (0.6877777777777778, 'agree')]\n",
      "96.41652613827993\n",
      "mosei02 selected models [(0.6477777777777778, 'agree'), (0.6488888888888888, 'align'), (0.6488888888888888, 'recon')]\n",
      "98.91414998303361\n",
      "sarcasm01 selected models [(0.6594444444444445, 'recon'), (0.6605555555555556, 'agree'), (0.6633333333333333, 'align')]\n",
      "100.0\n",
      "sarcasm02 selected models [(0.735, 'align'), (0.7355555555555555, 'agree'), (0.7438888888888889, 'recon')]\n",
      "100.0\n",
      "sarcasm12 selected models [(0.6844444444444444, 'early_fusion'), (0.685, 'recon'), (0.6877777777777778, 'agree')]\n",
      "96.90721649484544\n",
      "{'mimic': 0.9172286940527284, 'enrico': 0.5205479452054794, 'humor12': 0.636105860113421, 'humor01': 0.609640831758034, 'humor02': 0.648393194706994, 'mosei12': 0.8100082485565027, 'mosei01': 0.6521858674731922, 'mosei02': 0.810283200439923, 'sarcasm01': 0.615942028985507, 'sarcasm02': 0.695652173913043, 'sarcasm12': 0.702898550724637}\n"
     ]
    }
   ],
   "source": [
    "for setting in datasets:\n",
    "    tmp = []\n",
    "    for old_setting in old_datasets:\n",
    "        tmp.append((np.sum(np.absolute([normalized_datasets[setting][measure]-normalized_old_datasets[old_setting][measure] for measure in MEASURES])), old_setting))\n",
    "    tmp = sorted(tmp)\n",
    "    for i in range(1):\n",
    "        print(setting, \"selected models\", model_selection[tmp[i][1]])\n",
    "        selected_models = model_selection[tmp[i][1]]\n",
    "        model_scores = []\n",
    "        for _, method in selected_models:\n",
    "            if method == 'recon':\n",
    "                method = 'mfm'\n",
    "            if method in results[setting]:\n",
    "                model_scores.append((results[setting][method]['acc'], method))\n",
    "        model_scores = sorted(model_scores)\n",
    "        print(model_scores[-1][0] / model_selection_best[setting] * 100)\n",
    "print(model_selection_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8158f520b0615a91d72976457965394544e0f25ca15232774db0f5a21042574b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
